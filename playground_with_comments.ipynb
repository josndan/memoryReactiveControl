{
 "cells": [
  {
   "cell_type": "code",
   "id": "78add2af9010fb25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:49:43.019325Z",
     "start_time": "2024-12-01T22:49:41.265973Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from torchvision import transforms\n",
    "\n",
    "from gridverse_torch_featureextractors.gridversefeatureextractor import GridVerseFeatureExtractor\n",
    "# from gridverse_utils.custom_gridverse_env import register_custom_functions\n",
    "from gridverse_utils.gridversemaker import WorldMaker\n",
    "import tree\n",
    "import numpy as np\n",
    "\n",
    "# register_custom_functions()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:50:01.748455Z",
     "start_time": "2024-12-01T22:50:01.742712Z"
    }
   },
   "cell_type": "code",
   "source": "np.ones((5, 5, 5))[0, 1:].shape",
   "id": "57ec51cb566edaa7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28641179dc703a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:42.335837Z",
     "start_time": "2024-12-01T20:17:42.334437Z"
    }
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# \n",
    "# wandb.init(project=\"self-supervised-memory-reactive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498dff2dce7b38b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:42.399459Z",
     "start_time": "2024-12-01T20:17:42.336372Z"
    }
   },
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"./config/hydra_conf\", version_base=None)\n",
    "cfg = compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cde4818788e311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:43.278901Z",
     "start_time": "2024-12-01T20:17:43.275800Z"
    }
   },
   "outputs": [],
   "source": [
    "def numpy_dict_to_tensor(data):\n",
    "    return {k: torch.as_tensor(v) for k, v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ac78954dc972f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:43.536734Z",
     "start_time": "2024-12-01T20:17:43.528915Z"
    }
   },
   "outputs": [],
   "source": [
    "from gym_gridverse.action import Action\n",
    "\n",
    "\n",
    "def collect_episodes(env, num_episodes, max_step_len, test=False):\n",
    "    episodes = []\n",
    "    returns = []\n",
    "    last_num_episodes = 0\n",
    "\n",
    "    while len(episodes) < num_episodes:\n",
    "        episode = [numpy_dict_to_tensor(world.reset()[0])]\n",
    "        epi_return = 0\n",
    "\n",
    "        for i in range(max_step_len - 1):\n",
    "            # Andrea: what is all this about? why not just always random?\n",
    "            # This is creating biased dataset where it's always seeing the beacon at the start\n",
    "\n",
    "            #random action\n",
    "            if test and i < 4:\n",
    "                # Cannot reach exit by turning but definitely sees beacon in 5x5 grid\n",
    "                action = Action.TURN_RIGHT.value\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "            else:\n",
    "                action = world.action_space.sample()\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "            # obs = numpy_dict_to_tensor(obs)\n",
    "\n",
    "            # Andrea: episode only contains observations? should probably also contain actions\n",
    "            episode.append(obs)\n",
    "            epi_return += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        if terminated and not truncated:\n",
    "            # repeat last observation so that all episodes have max_step_len\n",
    "            last_obs = episode[-1]\n",
    "            while len(episode) < max_step_len:\n",
    "                episode.append(last_obs)\n",
    "\n",
    "            episode = tree.map_structure(lambda *steps_: np.stack(steps_, axis=0), *episode)\n",
    "\n",
    "            episodes.append(episode)\n",
    "            # Assumes that positive return means agent reached good exit otherwise it reached bad exit\n",
    "            # label 1 means good exit, 0 means bad exit\n",
    "            returns.append(1.0 if epi_return > 0 else 0.0)\n",
    "\n",
    "        if len(episodes) % 100 == 0 and len(episodes) > last_num_episodes:\n",
    "            print(f\"\\tCollected{len(episodes)}\")\n",
    "            last_num_episodes = len(episodes)\n",
    "\n",
    "    episodes = tree.map_structure(lambda *episodes_: np.stack(episodes_, axis=0), *episodes)\n",
    "    returns = np.stack(returns, axis=0)\n",
    "    return episodes, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d0bc1f9285a041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:44.011534Z",
     "start_time": "2024-12-01T20:17:44.009732Z"
    }
   },
   "outputs": [],
   "source": [
    "# episodes = tree.map_structure(lambda *episodes_: torch.stack(episodes_, dim=0), *episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87570d690fa385a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:44.358803Z",
     "start_time": "2024-12-01T20:17:44.354014Z"
    }
   },
   "outputs": [],
   "source": [
    "from gridverse_torch_featureextractors.stacked.transformerencoder import generate_square_subsequent_mask\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
    "\n",
    "\n",
    "class ReturnPredictorFromSequence(nn.modules.Module):\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, config: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gridverse_feature_extractor = GridVerseFeatureExtractor(observation_space, config.encoder)\n",
    "        self.feature_dim = config.encoder.grid_encoder.output_dim + \\\n",
    "                           config.encoder.agent_id_encoder.output_dim + \\\n",
    "                           config.encoder.items_encoder.layers[-1]\n",
    "        hidden_dim = config.lstm_cell_size\n",
    "        self.rnn = nn.LSTM(input_size=self.feature_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        # encoder_layer = TransformerEncoderLayer(d_model=self.feature_dim, nhead=8, batch_first=True)\n",
    "        # self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.feature_dim, out_features=1),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(in_features=128, out_features=64),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(in_features=64, out_features=32),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(in_features=32, out_features=1)\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, episodes):\n",
    "        # reversing the episode so RNN process steps from the end\n",
    "        # (num_batch/ num_episode, num_timesteps,..)\n",
    "        episodes = tree.map_structure(lambda dict_val: torch.flip(dict_val, dims=[1]), episodes)\n",
    "        num_batch, num_steps, *_ = next(iter(episodes.values())).size()\n",
    "\n",
    "        def combine_batch_and_time_dim(tensor):\n",
    "            _, _, *feature_dims = tensor.size()\n",
    "            return tensor.reshape(-1, *feature_dims)\n",
    "\n",
    "        episodes = tree.map_structure(combine_batch_and_time_dim, episodes)\n",
    "\n",
    "        features = self.gridverse_feature_extractor(episodes)\n",
    "        _, *feature_dims = features.size()\n",
    "\n",
    "        # batch_size, num_steps, feature_dim\n",
    "        features = features.reshape(num_batch, num_steps, *feature_dims)\n",
    "\n",
    "        mask = generate_square_subsequent_mask(num_steps)\n",
    "        transformer_output = self.transformer_encoder(features, mask=mask)\n",
    "\n",
    "        aggregated_output = transformer_output[:, -1, :]  # Take the last timestep\n",
    "\n",
    "        # out, (rnn_h_n, c_n) = self.rnn(features)\n",
    "        # rnn_h_n = rnn_h_n.squeeze()\n",
    "\n",
    "        outputs = self.linear(aggregated_output)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5170e9553606c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:44.900037Z",
     "start_time": "2024-12-01T20:17:44.896530Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReturnPredictorFromMemoryAndReactiveObs(nn.modules.Module):\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, config: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gridverse_feature_extractor = GridVerseFeatureExtractor(observation_space, config.encoder)\n",
    "        self.feature_dim = config.encoder.grid_encoder.output_dim + \\\n",
    "                           config.encoder.agent_id_encoder.output_dim + \\\n",
    "                           config.encoder.items_encoder.layers[-1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * self.feature_dim, out_features=128),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, memory_reactive_obs):\n",
    "        #(N,2,grid_verse_dim)\n",
    "        num_batch, num_obs, *_ = next(iter(memory_reactive_obs.values())).size()\n",
    "\n",
    "        def combine_batch_and_time_dim(tensor):\n",
    "            _, _, *feature_dims = tensor.size()\n",
    "            return tensor.reshape(-1, *feature_dims)\n",
    "\n",
    "        memory_reactive_obs = tree.map_structure(combine_batch_and_time_dim, memory_reactive_obs)\n",
    "\n",
    "        features = self.gridverse_feature_extractor(memory_reactive_obs)\n",
    "\n",
    "        _, *feature_dims = features.size()\n",
    "\n",
    "        # batch_size, num_steps, feature_dim\n",
    "        features = features.reshape(num_batch, num_obs, *feature_dims)\n",
    "        features = features.reshape(num_batch, -1)\n",
    "\n",
    "        outputs = self.linear(features)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be22132511a4965",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:45.394279Z",
     "start_time": "2024-12-01T20:17:45.391452Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    def __init__(self, episodes: dict, labels: np.array, transform=None):\n",
    "        self.episodes = episodes\n",
    "        self.num_episodes, self.rollout_len, *_ = episodes['grid'].shape\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_episodes * self.rollout_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        episode_index = idx // self.rollout_len\n",
    "        rollout_index = idx % self.rollout_len\n",
    "\n",
    "        memory_step_and_reactive_step = tree.map_structure(\n",
    "            lambda episodes_value: np.stack(\n",
    "                [episodes_value[episode_index, rollout_index], episodes_value[episode_index, self.rollout_len - 1]],\n",
    "                axis=0),\n",
    "            self.episodes)\n",
    "        sample = self.transform(memory_step_and_reactive_step)\n",
    "        label = self.labels[episode_index]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8b7db50fcafdc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:46.315850Z",
     "start_time": "2024-12-01T20:17:46.313121Z"
    }
   },
   "outputs": [],
   "source": [
    "# def custom_dataloader_collate(batch):\n",
    "#     combined_dict = {}\n",
    "#     samples, labels = zip(*batch)\n",
    "#     for key in samples[0].keys():\n",
    "#         combined_dict[key] = torch.stack([sample[key] for sample in samples])\n",
    "#     return combined_dict, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a16c5d01793117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:46.660106Z",
     "start_time": "2024-12-01T20:17:46.649130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gridverse using YAML in ./config/gridverse_conf/gv_empty.4x4.yaml\n"
     ]
    }
   ],
   "source": [
    "# env param\n",
    "max_step_len = 50\n",
    "num_train_episodes = 5000\n",
    "num_valid_episodes = 1000\n",
    "num_test_episodes = 1000\n",
    "env_name = 'gv_empty.4x4'\n",
    "worldMaker = WorldMaker(f'./config/gridverse_conf/{env_name}.yaml')\n",
    "world = worldMaker.make_env()\n",
    "\n",
    "# hyper param\n",
    "batch_size = 128  # this is number of episodes when using sequence and num time steps when not using sequence\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "# Andrea: remove all references to cutoff_mag\n",
    "cutoff_mag = 0.1\n",
    "\n",
    "# params\n",
    "aggregate_stats_every_n_batch = 50\n",
    "model_path = f'{env_name}.self_supervised_memory.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d938b470197274b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:47.260156Z",
     "start_time": "2024-12-01T20:17:47.258230Z"
    }
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert dict ndarrays in sample to dict Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return {k: torch.as_tensor(v) for k, v in sample.items()}\n",
    "\n",
    "\n",
    "composed = transforms.Compose([\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69ca27143e50926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:47.902533Z",
     "start_time": "2024-12-01T20:17:47.899468Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_and_save_episodes():\n",
    "    print(\"Collecting train episodes...\")\n",
    "    train_episodes, returns = collect_episodes(world, num_train_episodes, max_step_len)\n",
    "    train_dataset = EpisodeDataset(train_episodes, returns, transform=composed)\n",
    "    torch.save(train_dataset, f'{env_name}.train_dataset.pt')\n",
    "\n",
    "    print(\"Collecting validation episodes...\")\n",
    "    validation_episodes, returns = collect_episodes(world, num_valid_episodes, max_step_len)\n",
    "    validation_dataset = EpisodeDataset(validation_episodes, returns, transform=composed)\n",
    "    torch.save(validation_dataset, f'{env_name}.validation_dataset.pt')\n",
    "\n",
    "    print(\"Collecting test episodes...\")\n",
    "    test_episodes, returns = collect_episodes(world, num_test_episodes, max_step_len, test=True)\n",
    "    test_dataset = EpisodeDataset(test_episodes, returns, transform=composed)\n",
    "    torch.save(test_dataset, f'{env_name}.test_dataset.pt')\n",
    "\n",
    "# collect_and_save_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366ca32fdf6ac52e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:49.581233Z",
     "start_time": "2024-12-01T20:17:49.577093Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        # # unsqueeze and expand labels for time dim\n",
    "        # labels = labels[..., None].expand(*outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % aggregate_stats_every_n_batch == (aggregate_stats_every_n_batch - 1):\n",
    "            last_loss = running_loss / aggregate_stats_every_n_batch  # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "855c0e57f126b976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:17:51.014290Z",
     "start_time": "2024-12-01T20:17:50.175264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/7zkq50550235g1kcjw6b9j4h0000gn/T/ipykernel_43295/4219035015.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load(f'{env_name}.train_dataset.pt')\n",
      "/var/folders/7v/7zkq50550235g1kcjw6b9j4h0000gn/T/ipykernel_43295/4219035015.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  validation_dataset = torch.load(f'{env_name}.validation_dataset.pt')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.load(f'{env_name}.train_dataset.pt')\n",
    "validation_dataset = torch.load(f'{env_name}.validation_dataset.pt')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, )\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, )\n",
    "\n",
    "model = ReturnPredictorFromMemoryAndReactiveObs(world.observation_space, cfg.algorithm)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3eb128beb7b973",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-01T20:17:53.560231Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "EPOCH 1:\n",
      "  batch 50 loss: 0.11016565950858377\n",
      "  batch 100 loss: 0.0029446894159822663\n",
      "  batch 150 loss: 0.0008583186636231848\n",
      "  batch 200 loss: 0.002515019514846445\n",
      "  batch 250 loss: 0.0020344744428884357\n",
      "  batch 300 loss: 0.0008771065909873332\n",
      "  batch 350 loss: 0.0012997926096253167\n",
      "  batch 400 loss: 0.00012946882809720847\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"Starting training...\")\n",
    "best_vloss = float(\"inf\")\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_vloss += loss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # wandb.log({'train/loss': avg_loss, 'valid/loss': avg_vloss}, epoch_number + 1)\n",
    "    epoch_number += 1\n",
    "    # epoch_number += i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff33cfe181a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.load(f'{env_name}.test_dataset.pt')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, )\n",
    "model = ReturnPredictorFromMemoryAndReactiveObs(world.observation_space, cfg.algorithm)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805814d389d384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d0446e132f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the model's prediction after it has seen the first step.\n",
    "        # The test dataset definitely contains the Memory observation.\n",
    "        outputs = torch.sigmoid(outputs.squeeze())\n",
    "        res.extend(outputs.cpu().numpy())\n",
    "        # note this doesn't calculate the accuracy for the class when the model is unsure.\n",
    "        # this is accuracy for 0,1 class\n",
    "\n",
    "        # Andrea: never do these hard thresholds, it's hiding what the model is actually doing\n",
    "\n",
    "        pos_threshold = 1 - cutoff_mag\n",
    "        neg_threshold = cutoff_mag\n",
    "\n",
    "        outputs[outputs < neg_threshold] = 0\n",
    "        outputs[outputs > pos_threshold] = 1\n",
    "        outputs[torch.logical_and(outputs >= neg_threshold, outputs <= pos_threshold)] = 0.5\n",
    "        # Andrea: the thing you want, without thresholding is:\n",
    "        # total_correct += (labels * outputs + (1-labels) * (1 - outputs)).sum().item()\n",
    "        total_correct += torch.sum(outputs == labels).item()\n",
    "        total_samples += outputs.numel()\n",
    "    one_or_zero_class_acc = total_correct / total_samples\n",
    "    # wandb.log({\"test/one_or_zero_class_acc\": one_or_zero_class_acc})\n",
    "    print(one_or_zero_class_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d398f4ab9250ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025130ed2cbfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8682a39e844858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e94bde270550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(res, bins=100, range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03bea75eb7d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), align=\"edge\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b359aafdc66cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924de653e071ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_len = 50\n",
    "num_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7400f3b769fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_learnable_memory_reactive_pair_per_episode = 1  # This could be more or less than this. \n",
    "num_memory_reactive_pair_per_episode = rollout_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0eeb4f4fe1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_learnable_pair = num_learnable_memory_reactive_pair_per_episode * num_episodes\n",
    "tot_pair = num_memory_reactive_pair_per_episode * num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4d4b2a96b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_learnable_pair, tot_pair, tot_learnable_pair / tot_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9553d33744092",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1 - tot_learnable_pair / tot_pair\n",
    "f'noise: {noise * 100:0.2f} %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c5695801f1515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
