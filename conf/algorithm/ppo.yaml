activation_fn: Tanh
use_sde: False # Whether to use state-dependent exploration
use_expln: False # Whether to use exponential linear units

feature_extractor:
  share_feature_extractor: True
  grid_encoder:
    embedding_dim: 32
    type: cnn
    output_dim: 32
    conv_layers:
      - in_channels: ${...embedding_dim}
        out_channels: 32
        kernel_size: 3
        stride: 1
        padding: 0
  agent_id_encoder:
    type: cnn
    output_dim: 32
    conv_layers:
      - in_channels: 1
        out_channels: 32
        kernel_size: 3
        stride: 1
        padding: 0
  items_encoder:
    embedding_dim: 8
    layers: [ 64, 32 ]

lstm:
  enable_critic_lstm: False
  shared_lstm: False # Whether to share the lstm between actor and critic
  lstm_hidden_size: 256
  n_lstm_layers: 2

mlp:
  policy_net:
    lstm_output_to_latent_features: [ 128, 64, 32 ]
  value_net:
    lstm_output_to_latent_features: [ 128, 64, 32 ]

training:
  num_env_steps_for_each_gradient_update: 256
  num_epochs_optimizing_surrogate_loss: 10
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: True
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null

testing:
  max_episode_steps: 25
  n_eval_episodes: 100
